# Operation: 
          Repeat k different actions
          After each choice, you get a numerical reward
          Objective: Maximize the expected total reward over some time period

# Problem Description:
         +> There are k different actions. Each of the k actions has an expected or mean reward given that that action is selected; 
          let us call this the value of that action. 
         +> We denote the action selected on time step t as At, and the corresponding reward as Rt
         +> The value of an arbitrary action a, denoted q*(a), is the expected reward given that a is selected, defined by the formula
                              q*(a) = E[ Rt | At = a ]
                    => always select the action with the highest value.

But Get more difficult: Case that do not know the action value 
                    => Denote estimated value as Qt(a)
                    => Objective that Qt(a) to be close to q*(a)

Greedy action: The action that maintain estimates of the action values 
          -> always have at least one action whose estimated value is greatest.
Nongreedy action: The action that not a greedy action.

          -> Using the hardest case in RL: Exploiting or exploring (Khai thác hay thăm dò)
                    - Exploiting: is to select one of GA -> for initial run
                    - Exploring: is to select of of NGA. -> for long run  

==>> We haven't found the best solution to choose exploiting or exploring cause it depends on some aspects
like estimates, uncertainties and the number of remaining steps.

Most of these methods make strong assumptions about stationarity (tính dừng - phần thưởng trung bình
cho mỗi lượt là không thay đổi) and prior knowledge (kiến thức tiên nghiệm) that are either violated or impossible to verify in most applications and in
the full reinforcement learning problem that we consider in subsequent chapters. The
guarantees of optimality or bounded loss for these methods are of little comfort when the
assumptions of their theory do not apply

Bonus: Cơ chế hoạt động của K-A Bandit
          - Sau ước tính ban đầu về "giá trị" của từng phần tử k, thông qua policy mà agent sẽ đưa ra 
          action và nhận lại reward. Reward đó đóng vai trò ước tính lại value cho action đã chọn
          -> Lặp lại vòng này để sau mỗi ước tính Qt(a) sẽ đến gần hơn với giá trị thực q*(a) cần -> đưa ra
          các lựa chọn tốt nhất

So.... when do we use this position ?






       
