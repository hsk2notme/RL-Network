# Operation: 
          Repeat k different actions
          After each choice, you get a numerical reward
          Objective: Maximize the expected total reward over some time period

# Problem Description:
         +> There are k different actions. Each of the k actions has an expected or mean reward given that that action is selected; 
          let us call this the value of that action. 
         +> We denote the action selected on time step t as At, and the corresponding reward as Rt
         +> The value of an arbitrary action a, denoted q*(a), is the expected reward given that a is selected, defined by the formula
                              q*(a) = E[ Rt | At = a ]
                    => always select the action with the highest value.

But Get more difficult: Case that do not know the action value 
                    => Denote estimated value as Qt(a)
                    => Objective that Qt(a) to be close to q*(a)

Greedy action: The action that maintain estimates of the action values 
          -> always have at least one action whose estimated value is greatest.
Nongreedy action: The action that not a greedy action.

          -> Using the hardest case in RL: Exploiting or exploring (Khai thác hay thăm dò)
                    - Exploiting: is to select one of GA -> for initial run
                    - Exploring: is to select of of NGA. -> for long run  

==>> We haven't found the best solution to choose exploiting or exploring cause it depends on some aspects
like estimates, uncertainties and the number of remaining steps.

Most of these methods make strong assumptions about stationarity (tính dừng - phần thưởng trung bình
cho mỗi lượt là không thay đổi) and prior knowledge that are either violated or impossible to verify in most applications and in
the full reinforcement learning problem that we consider in subsequent chapters. The
guarantees of optimality or bounded loss for these methods are of little comfort when the
assumptions of their theory do not apply

When do we use this position ?

Khi phải lựa chọn giữa nhiều phương án cạnh tranh: 
Cốt lõi của bài toán là phải chọn ra một trong k lựa chọn có sẵn (ví dụ: các phiên bản quảng cáo, các loại thuốc, các tiêu đề bài báo).

Khi kết quả của mỗi lựa chọn là không chắc chắn: Bạn không biết chắc chắn phần thưởng nhận được mỗi khi lựa chọn là bao nhiêu, nó tuân theo một phân phối xác suất nào đó mà bạn phải tìm hiểu.

Khi cần tối ưu hóa kết quả theo thời gian: Mục tiêu không phải là tìm ra lựa chọn tốt nhất trong một lần duy nhất, mà là tối đa hóa tổng lợi ích thu được qua một chuỗi các quyết định.

Khi việc thu thập thông tin tốn chi phí: Cách duy nhất để biết một lựa chọn tốt hay không là phải thử nó. Mỗi lần "thử" (thăm dò) một lựa chọn không tối ưu, bạn sẽ mất đi một khoản lợi ích mà lẽ ra bạn có được nếu chọn phương án tốt nhất đã biết (khai thác). Đây chính là "chi phí" của việc học hỏi.

Khi các lựa chọn tương đối độc lập và không ảnh hưởng đến trạng thái tương lai: Đây là điểm khác biệt chính so với học tăng cường đầy đủ. Việc chọn quảng cáo A hôm nay không làm thay đổi bản chất của quảng cáo B vào ngày mai. Vấn đề mang tính "phi liên kết" (non-associative), tức là không có khái niệm "trạng thái" phức tạp.

Tóm lại, K-armed Bandit là mô hình lý tưởng cho các bài toán tối ưu hóa lựa chọn trong điều kiện không chắc chắn, nơi tồn tại sự đánh đổi trực tiếp giữa việc khai thác phương án tốt nhất hiện tại và việc thăm dò để tìm ra phương án tốt hơn trong tương lai.


          
